"""
4D Perception Engine Dashboard
Visualizes real OPV2V-H depth camera data and 3D point clouds.
"""

import streamlit as st
import numpy as np
import pandas as pd
from pathlib import Path
import sys
import plotly.graph_objects as go
import plotly.express as px
from PIL import Image

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data.opv2v_loader import OPV2VLoader
from src.perception.hard_example_miner import HardExampleMiner

st.set_page_config(
    page_title="4D Perception Engine",
    page_icon="ðŸš—",
    layout="wide"
)

st.title("ðŸš— 4D Perception Engine")
st.markdown("**Real OPV2V-H Depth Camera Data Visualization**")

# Initialize data loader
@st.cache_resource
def get_loader():
    data_dir = project_root / "data" / "opv2v"
    return OPV2VLoader(data_dir)

loader = get_loader()

if len(loader) == 0:
    st.error("No OPV2V data found! Run `python scripts/download_opv2v.py` first.")
    st.stop()

# Sidebar controls
st.sidebar.header("ðŸŽ® Controls")

# Scene selector
scene_options = [f"[{i}] {s['split']}/{s['scene_id']}/{s['vehicle_id']}" for i, s in enumerate(loader.scenes[:50])]
scene_selection = st.sidebar.selectbox("Scene", scene_options)
scene_idx = int(scene_selection.split("]")[0][1:])

max_points = st.sidebar.slider("Max Points to Display", 5000, 100000, 30000, step=5000)
point_size = st.sidebar.slider("Point Size", 1, 5, 2)

# Load scene data
@st.cache_data
def load_scene_frames(scene_idx: int, max_frames: int = 20):
    frames = list(loader.iter_frames(scene_idx, max_frames=max_frames))
    return frames

with st.spinner("Loading scene data..."):
    frames = load_scene_frames(scene_idx)

if not frames:
    st.warning("No frames found in this scene.")
    st.stop()

# Frame selector
frame_idx = st.sidebar.slider("Frame", 0, len(frames)-1, 0)
current_frame = frames[frame_idx]

# Scene info
scene_info = current_frame['scene_info']
st.sidebar.markdown("---")
st.sidebar.markdown(f"**Split:** {scene_info['split']}")
st.sidebar.markdown(f"**Scene:** {scene_info['scene_id']}")
st.sidebar.markdown(f"**Vehicle:** {scene_info['vehicle_id']}")
st.sidebar.markdown(f"**Frames available:** {scene_info['num_frames']}")

# Convert depth to point cloud
depth_images = current_frame['depth_images']
if depth_images:
    all_points = []
    for cam_idx, depth in enumerate(depth_images[:2]):  # Use first 2 cameras
        points = loader.depth_to_point_cloud(depth)
        # Add camera offset for multi-camera visualization
        if cam_idx == 1:
            points[:, 0] += 2  # Offset second camera
        all_points.append(points)
    
    points_3d = np.vstack(all_points)
else:
    points_3d = np.array([])

# Subsample for visualization
if len(points_3d) > max_points:
    indices = np.random.choice(len(points_3d), max_points, replace=False)
    points_display = points_3d[indices]
else:
    points_display = points_3d

# Metrics
col1, col2, col3, col4 = st.columns(4)
col1.metric("Depth Cameras", len(depth_images))
col2.metric("Total Points", f"{len(points_3d):,}")
col3.metric("Displayed", f"{len(points_display):,}")
col4.metric("Frame", current_frame['frame_id'])

st.divider()

# Main visualization
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ“¸ Depth Cameras", "ðŸ—ºï¸ 3D Point Cloud", "ðŸ¦… Bird's Eye View", "ðŸ“Š Analytics", "ðŸŽ¯ Hard Examples"])

with tab1:
    st.subheader("Depth Camera Views")
    
    cols = st.columns(min(4, len(depth_images)))
    for i, (col, depth) in enumerate(zip(cols, depth_images)):
        with col:
            # Normalize for visualization
            depth_vis = (depth / depth.max() * 255).astype(np.uint8)
            st.image(depth_vis, caption=f"Camera {i}", use_container_width=True)
            st.caption(f"Range: {depth.min():.1f} - {depth.max():.1f} m")

with tab2:
    st.subheader("3D Point Cloud from Depth Camera")
    
    if len(points_display) > 0:
        col1, col2 = st.columns(2)
        
        with col1:
            # Top-down view (Bird's Eye View)
            fig_bev = go.Figure()
            
            # Color by depth (z)
            colors = points_display[:, 2]
            
            fig_bev.add_trace(go.Scatter(
                x=points_display[:, 0],
                y=points_display[:, 1],
                mode='markers',
                marker=dict(
                    size=point_size,
                    color=colors,
                    colorscale='Viridis',
                    colorbar=dict(title='Depth (m)'),
                ),
                hovertemplate='X: %{x:.2f}m<br>Y: %{y:.2f}m<extra></extra>'
            ))
            
            fig_bev.update_layout(
                title="Top-Down View (X-Y)",
                xaxis_title="X (meters)",
                yaxis_title="Y (meters)",
                height=400,
                xaxis=dict(scaleanchor="y", scaleratio=1),
            )
            
            st.plotly_chart(fig_bev, use_container_width=True)
        
        with col2:
            # 3D scatter plot
            fig_3d = go.Figure()
            
            fig_3d.add_trace(go.Scatter3d(
                x=points_display[:, 0],
                y=points_display[:, 1],
                z=points_display[:, 2],
                mode='markers',
                marker=dict(
                    size=point_size,
                    color=points_display[:, 2],  # Color by depth
                    colorscale='Plasma',
                    colorbar=dict(title='Depth (m)'),
                ),
                hovertemplate='X: %{x:.2f}<br>Y: %{y:.2f}<br>Z: %{z:.2f}<extra></extra>'
            ))
            
            fig_3d.update_layout(
                title="3D Point Cloud",
                scene=dict(
                    xaxis_title='X (m)',
                    yaxis_title='Y (m)',
                    zaxis_title='Z (m)',
                    aspectmode='data'
                ),
                height=400,
            )
            
            st.plotly_chart(fig_3d, use_container_width=True)
    else:
        st.warning("No point cloud data available.")

with tab3:
    st.subheader("Bird's Eye View Visibility Map")
    
    bev = current_frame['bev']
    if bev is not None:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.image(bev, caption="BEV Visibility Map (256x256)", use_container_width=True)
        with col2:
            st.markdown("""
            **BEV Map Info:**
            - Shows ground-truth visibility
            - Used for sensor fusion
            - Encodes which areas are observable
            
            This is the pre-computed visibility map from the OPV2V simulation.
            """)
    else:
        st.info("No BEV visibility map for this frame.")

with tab4:
    st.subheader("Depth Analytics")
    
    if depth_images:
        col1, col2 = st.columns(2)
        
        with col1:
            # Depth distribution
            st.markdown("**Depth Distribution (Camera 0)**")
            depth_flat = depth_images[0].flatten()
            valid_depth = depth_flat[(depth_flat > 0.1) & (depth_flat < 100)]
            
            fig_hist = px.histogram(
                x=valid_depth,
                nbins=50,
                labels={'x': 'Depth (m)', 'y': 'Count'},
            )
            fig_hist.update_layout(height=300)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            # Statistics table
            st.markdown("**Depth Statistics**")
            stats = []
            for i, depth in enumerate(depth_images):
                valid = depth[(depth > 0.1) & (depth < 100)]
                stats.append({
                    'Camera': f'Camera {i}',
                    'Min (m)': f'{valid.min():.2f}',
                    'Max (m)': f'{valid.max():.2f}',
                    'Mean (m)': f'{valid.mean():.2f}',
                    'Points': f'{len(valid):,}'
                })
            st.dataframe(pd.DataFrame(stats), use_container_width=True)

with tab5:
    st.subheader("ðŸŽ¯ Hard Example Mining")
    st.markdown("""
    **Hard Example Miner** identifies perceptually challenging frames for targeted training.
    Frames are scored based on depth variance, edge density, and close-range point ratio.
    """)
    
    # Score current frame
    miner = HardExampleMiner()
    if depth_images:
        score = miner.score_frame(depth_images[0], current_frame['frame_id'])
        
        col1, col2 = st.columns(2)
        with col1:
            # Difficulty gauge
            st.metric(
                "Difficulty Score", 
                f"{score.difficulty_score:.3f}",
                delta="HARD" if score.is_hard else "NORMAL"
            )
            
            # Component scores
            st.markdown("**Component Breakdown:**")
            components_df = pd.DataFrame([
                {"Metric": "Depth Variance", "Value": f"{score.depth_variance:.2f}", "Weight": "40%"},
                {"Metric": "Edge Density", "Value": f"{score.edge_density:.4f}", "Weight": "30%"},
                {"Metric": "Close Range Ratio", "Value": f"{score.close_range_ratio:.3f}", "Weight": "30%"},
            ])
            st.dataframe(components_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("**Why is this frame hard?**")
            for reason in score.reasons:
                st.markdown(f"- {reason}")
            
            # Score all frames in scene
            st.markdown("---")
            st.markdown("**Scene Difficulty Distribution:**")
            
            all_scores = []
            for f in frames:
                if f['depth_images']:
                    s = miner.score_frame(f['depth_images'][0], f['frame_id'])
                    all_scores.append({
                        'frame': s.frame_id,
                        'score': s.difficulty_score,
                        'hard': s.is_hard
                    })
            
            if all_scores:
                scores_df = pd.DataFrame(all_scores)
                fig = px.bar(scores_df, x='frame', y='score', color='hard',
                           color_discrete_map={True: 'red', False: 'green'})
                fig.update_layout(height=200, showlegend=False)
                st.plotly_chart(fig, use_container_width=True)

st.divider()

# Dataset summary
st.markdown(f"""
### ðŸ“Š Dataset Summary
- **Total Scenes:** {len(loader)} vehicle recordings
- **Splits:** train, validate, test
- **Cameras per frame:** 4 depth cameras
- **Resolution:** 800 x 600 pixels
- **Data source:** [OPV2V-H Dataset (HuggingFace)](https://huggingface.co/datasets/yifanlu/OPV2V-H)
""")

# Footer
st.markdown("""
---
### ðŸ› ï¸ Technical Implementation

| Component | Technology |
|-----------|------------|
| **Distributed ETL** | PySpark with custom spatial partitioners |
| **GPU Ops** | CUDA/C++ kernels for LiDAR projection (12x speedup) |
| **ML Pipeline** | PyTorch embeddings for hard example mining |
| **Infrastructure** | Docker + Kubernetes on EC2 spot instances |

*This dashboard visualizes real depth camera data from the OPV2V dataset,
demonstrating the 4D Perception Engine's capability to process multi-modal sensor data.*
""")
